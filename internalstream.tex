\section{Support for Large Number of Streams Using Internal Streams}
{\color{blue}
As explained in Section 2.2, the number of streams is restricted to a small number 
because of the practical limits on the backup power capacity and the size of fast memory.  
However, if additional operating conditions are guaranteed, special streams may be 
supported more easily over normal streams.  
In this Section, we focus on such  a special stream type, called {\it internal} streams, which
are used only for copying data internally during garbage collection.  
Internal streams are efficiently implemented while they significantly 
improve the efficiency of PC-based stream allocation.
}

\subsection{Program contexts with large lifetime variances}
For most PCs, their lifetime distributions tend to have small variances.  
However, we observed that a few outlier PCs which have large lifetime variations. 
For example, when multiple I/O contexts are covered by the same execution path, 
the corresponding PC may represent several I/O contexts whose data lifetimes are quite different.   
Such a case occurs, for example, 
in the compaction module of RocksDB.
RocksDB maintains
several levels, L1, ..., L$n$, in the persistent storage, except for L0 (or a
memtable) stored in DRAM.  Once one level, say L2, becomes full, all the data
in L2 is compacted to a lower level, i.e., L3.  It involves moving data from L2
to L3, along with the deletion of the old data in L2.  In the
LSM tree~\cite{LSM}, a higher level is smaller than a lower level 
(i.e., the size of (L2) $<$ the size of (L3)). 
Thus, data stored in a higher level is invalidated more frequently than those kept
in lower levels, thereby having shorter lifetimes.

%Once the L1 becomes full,
%\textit{all} the data kept in the L1 are moved to the L2 by the compaction
%module.  The same operation is applied to the other levels (i.e., L3, ...,
%L$n-1$).  The compaction involves reading and writing data from a higher level
%(e.g., L1) to a lower level (e.g., L2).  The data in a higher level (e.g., L1)
%is then removed.  

%While the program context can be used as a useful indicator that determines the
%lifetime of data, we also observe that the same PC could generate data 
%with diverged lifetimes. One of the representative examples is the compaction
%module of RocksDB. RocksDB maintains several levels, L1, ..., L$n$, in the
%persistent storage, except for L0 (or a memtable) stored in DRAM.  Data flushed
%from the memtable are first written to the L1.  Once the L1 becomes full,
%\textit{all} the data kept in the L1 are moved to the L2 by the compaction
%module.  The same operation is applied to the other levels (i.e., L3, ...,
%L$n-1$).  The compaction involves reading and writing data from a higher level
%(e.g., L1) to a lower level (e.g., L2).  The data in a higher level (e.g., L1)
%is then removed.  In the LSM-tree, a higher level is smaller than a lower
%level. Thus, data stored in a higher level is invalidated sooner than data kept
%in lower levels, thereby having much shorter lifetimes.

Unfortunately, in the current RocksDB implementation, the compaction step is supported 
by the same execution path (i.e., the same PC) regardless of the level.
Therefore, the PC for the compaction activity cannot effectively separate data with 
short lifetimes from one with long lifetimes.
Fig.~\ref{fig:largevariance}(a) shows 
the lifetime distribution collected from the compaction-activity PC.  
{\color{red}TODO: 추가 그림에 대한설명 \\ \\ \\ \\ }

Since it is difficult to separate data with different lifetimes within the same PC 
(as in the compaction-activity PC), we devised a two-phase method that decides SSD 
streams in two levels: the main stream in the host level and 
its internal stream in the SSD level.
Conceptually, long-lived data in the main stream are moved to its internal stream to 
separate from (future) short-lived data of the main stream.
Although moving data to the internal stream may increase WAF,
the overhead can be hidden if we restrict data copies to the internal stream during GC only.
Since long-lived data (i.e., valid pages) in a victim block are moved to a free block during GC, 
blocks belong to an internal block tend to contain long-lived data.
For instance, \textsf{\small PCStream} assigns the compaction-activity PC {\it $PC_1$} to a
main stream {\it $S_1$} in the first phase.
To separate the long-lived data of {\it $PC_1$} (e.g., L4 data) 
from future short-lived data of the same {\it $PC_1$} (e.g., L1 data), 
valid pages of the {\it $S_1$} are assigned to its internal stream for the second phase during GC.
{\color{blue}
In order to efficiently support the proposed two-phase method, the total number of streams 
should be doubled including normal streams and internal streams.
}


\begin{comment}
Since this distribution includes lifetimes of data written from all the levels, 
its variance is large.  
When we manually separate the single compaction step into several per-level compaction steps, 
as shown in Figs. 5(b) and 5(c), the lifetime distributions of per-level compaction steps 
show smaller variances.   
In particular, L2 and L3 show distinct lifetime distributions from that of L4.
Data from L2 and L3 are likely to have shorter lifetimes, while L4 has generally
long-lived data as shown in Fig. 5(d).
\end{comment}

\begin{figure}[!t]
\centering
%\vspace{-7pt}
\hfill
\subfloat[RocksDB: compaction]{\includegraphics[width=0.2\textwidth]{figure/type_6}}% data from 4/03040047
	\hspace{2pt}
\subfloat[RocksDB: L2]{\includegraphics[width=0.2\textwidth]{figure/type_4}}  
\hfill
\vspace{7pt}
\subfloat[sqlite: data files]{\includegraphics[width=0.2\textwidth]{figure/sqlite_long_LBA}}
	\hspace{2pt}
\subfloat[gcc: result files]{\includegraphics[width=0.2\textwidth]{figure/compile_long}}
%\vspace{-10pt}
%\caption{The lifetime distribution of the compaction activity.} 
\caption{Large lifetime variance I/O activities for various workloads.} %shane part
\label{fig:largevariance}
%\vspace{-20pt}
\end{figure}

\subsection{Efficient Implementation of Internal Streams}

{\color{blue}
As described in Section 2.2, it is difficult to increase the number of 
(normal) streams.  However, since internal streams are limited for 
data movements during GC only, they can be quite efficiently
implemented without the constraints on the backup power capacity and fast memory size.  
The key difference in the implementation overhead between normal streams and 
internal streams comes from a simple observation that data copied during 
garbage collection do not need the same reliability and performance support for host writes.  
Unlike buffered data from host write requests, valid pages in
the source block during garbage collection have no risk of losing their data 
from the sudden power-off conditions because the original valid pages are always available.    
Therefore, even if the
number of internal streams increases, unlike normal streams, there is no requirement 
for a higher-capacity backup capacitor for managing buffered data for internal streams. 
In Section 2.2, we describe various resource overheads that limit the number of external streams.
In the case of an internal stream used for internal data migration of SSD, 
it has a relatively small resource overhead due to its distinguishing characteristics from the external stream
and it can be used to manage the lifetime variance within streams.

The fast memory requirement is also not directly increased as the number 
of internal streams increases.   
Since internal streams are used only for GC and most GC can be handled as background tasks,
internal streams has a less stringent performance requirement.  
Therefore, data structures for supporting internal streams can be placed 
on DRAM without much performance issues.  
Furthermore, for a read request, there is no need to check if a read request 
can be served by buffered data as in normal streams because the source block always 
has the most up-to-date data.  
This, in turn, allows data structures for internal streams to be located in slow memory.

Once an SSD reaches the fully saturated condition where host writes and GC 
are concurrently performed, the performance of GC may suffer some performance degradation 
from the slow DRAM used for internal streams.   
However, in our evaluation, such cases were rarely observed with a reasonable overprovisioning capacity.
}

\begin{comment}
{\color{blue}
With regard to power resources, the buffering mechanism can be used for internal streams to maximize flash parallelism, but there is a big difference that no power resource is required to guarantee data integrity of buffered data.
Buffered data of host write will be lost if the data is not stored during power off handling.
However, the buffered data of internal migration doesn't need to be saved during power off handling. Because the original data always exists in the source block of migration. 
Therefore, there is no problem in ensuring data integrity without special handling or power resource requirement.

Unlike external streams, which require host requests to be handled directly in the foreground,
internal streams can be handled as background operations, 
so even if the data structures of internal streams are located in relatively slow memory, 
host request processing performance may not suffer. 
And the data structures for internal streams do not need to be checked during 
read request processing.
This is due to the fact that the data managed by the internal stream is not up-to-date 
and the source block has the same data. 
So read performance is not affected by usage of slow memeory for internal streams.
Under saturated conditions, which require concurrent processing of GC while processing a host write request, 
performance degradation may occur if slower memory is used in the internal stream, but performance degradation is small compared to using slow memory for external streams.
}
\end{comment}




