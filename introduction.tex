\section{Introduction}
\label{sec:intro}
In flash-based SSDs, garbage collection (GC) is inevitable because NAND flash 
memory does not support in-place updates.  
Since the efficiency of garbage collection significantly affects  
both the performance and lifetime of SSDs, garbage collection has been extensively 
investigated so that the garbage collection overhead can be reduced
~\cite{GCGreedy, GCVictim, GCTTFlash, HotCold}.  
For example, hot-cold separation techniques are commonly used inside an SSD 
so that quickly invalidated pages are not mixed with long-lived data in the same block.   
For more efficient garbage collection, many techniques also exploit
host-level I/O access characteristics which can be used as useful hints on 
the efficient data separation inside the SSD~\cite{JiTGC, ShadowGC}.

Multi-streamed SSDs provide a special interface mechanism for 
a host system, called streams,  with which data separation decisions 
on the host level can be delivered to SSDs~\cite{T10, MultiStream}.  
When the host system assigns two data {\it D1} and {\it D2} to 
different streams $S_1$ and $S_2$, respectively, a multi-streamed SSD 
places $D_1$ and $D_2$ in different blocks, which belong to S1 and S2, respectively.
When $D_1$ and $D_2$ have distinct update patterns, say, $D_1$ with a short lifetime 
and $D_2$ with a long lifetime, allocating $D_1$ and $D_2$ to different streams 
can be effective in minimizing the copy cost of
garbage collection by separating hot data from cold data.  
When streams are properly managed, they can significantly
improve both the performance and lifetime of 
flash-based SSDs~\cite{MultiStream, Level, FStream, vStream, AutoStream}.

In order to fully exploit the potential benefit of multi-streamed SSDs, 
a fully automatic management of streams is essential without requiring any 
code changes in applications.  For example, if an
application developer should manage stream allocations {\it manually} for 
a given SSD with $m$ streams, multi-streamed SSDs are difficult to be 
widely employed in practice.   If the application
developer has no expertise on the application's I/O characteristics or 
the underlying file system, it will be quite challenge to allocate streams manually.   
Furthermore, if manual stream allocation
exploits the number of available streams (i.e., $m$) in making stream allocation decisions,  
the same manual procedure should be repeated whenever the number of streams in an SSD changes.
In this paper, our goal is to develop 
a {\it fully automatic} technique for managing streams which 
needs no code change in applications regardless of multi-streamed SSDs used.

No existing techniques, however, meet our requirement.  
Most existing techniques~\cite{MultiStream, Level, FStream, vStream} requires manual work 
in assigning streams at the application level.  For example,  in both \textsf{\small vStream} and
\textsf{\small Samsung'OriginalTechnique}, it is an application developer's responsibility to assign streams 
to the application's I/O requests.  To the best of our knowledge, 
\textsf{\small AutoStream}~\cite{AutoStream} is the only known technique
that supports stream management in the kernel level without manual stream allocation.
However, since \textsf{\small AutoStream} predicts data lifetimes using the update frequency 
of the logical block address (LBA), it does not work well with modern append-only workloads 
such as RocksDB~\cite{RocksDB} or Cassandra~\cite{Cassandra}.  
Unlike conventional update workloads where data written to the same LBAs 
often show strong update locality, 
append-only workloads make it impossible to predict data lifetimes 
from LBA characteristics (such as access frequency or access patterns).  

In this paper, we propose a fully automatic stream management technique, called \textsf{\small PCStream}, 
for multi-streamed SSDs based on program contexts (PCs).
Since the key motivation behind \textsf{\small PCStream} was 
that data lifetimes should be estimated at a higher abstraction level than LBAs, 
\textsf{\small PCStream} employed a write program context\footnote{Since we are interested in write-related 
system call such as write() in the Linux kernel, 
we call the related program context as 
{\it write program contexts} or simply {\it program contexts} in this paper.}  
as a stream management unit.
A program context~\cite{PC, PC2}, which represents a particular execution phase of a program, 
is known to be an effective hint in separating data with different lifetimes~\cite{PCHa}.  
\textsf{\small PCStream} automatically maps an identified program context to a stream.  
Since program contexts can be computed during runtime, 
\textsf{\small PCStream} does not need any manual work.   
In order to handle append-only workloads, 
\textsf{\small PCStream} extended the definition of the data lifetime 
so that the effect of the TRIM command~\cite{TRIM} can be accounted for. 

Although most program contexts show that their data lifetimes are 
distributed with small variances, we observed a few outliers 
whose data lifetimes have rather large variances.
In \textsf{\small PCStream}, 
when such a PC {\it pID} is observed (which was mapped to a stream {\it sID}), 
the long-lived data of {\it pID} are moved to the substream of {\it sID}
during GC.  
The substream prevents the long-lived data of the stream {\it sID} 
from being mixed with future short-lived data of the stream {\it sID}.

In order to evaluate the effectiveness of \textsf{\small PCStream}, 
we have implemented \textsf{\small PCStream}
in the Linux kernel (ver. 4.5) and measured write amplification factor (WAF) values 
using RocksDB on a Samsung PM963 SSD 
and an SSD emulator.
Our experimental results show that \textsf{\small PCStream}
can reduce the GC overhead as much as a highly-optimized 
manual stream management technique while requiring no code modification.  
Furthermore, \textsf{\small PCStream} outperformed \textsf{\small AutoStream} by reducing the average WAF by 35\%.

The rest of this paper is organized as follows. 
We explain the key motivations behind \textsf{\small PCStream} in Section 2. 
Section 3 describes 
the design of \textsf{\small PCStream}.
The experimental results are shown in Section 4. 
Finally, we conclude in Section 5 with a summary and future work. 

